:py:mod:`dacapo.experiments.tasks.evaluators`
=============================================

.. py:module:: dacapo.experiments.tasks.evaluators


Submodules
----------
.. toctree::
   :titlesonly:
   :maxdepth: 1

   binary_segmentation_evaluation_scores/index.rst
   binary_segmentation_evaluator/index.rst
   dummy_evaluation_scores/index.rst
   dummy_evaluator/index.rst
   evaluation_scores/index.rst
   evaluator/index.rst
   instance_evaluation_scores/index.rst
   instance_evaluator/index.rst


Package Contents
----------------

Classes
~~~~~~~

.. autoapisummary::

   dacapo.experiments.tasks.evaluators.DummyEvaluationScores
   dacapo.experiments.tasks.evaluators.DummyEvaluator
   dacapo.experiments.tasks.evaluators.EvaluationScores
   dacapo.experiments.tasks.evaluators.Evaluator
   dacapo.experiments.tasks.evaluators.MultiChannelBinarySegmentationEvaluationScores
   dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores
   dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluator
   dacapo.experiments.tasks.evaluators.InstanceEvaluationScores
   dacapo.experiments.tasks.evaluators.InstanceEvaluator




.. py:class:: DummyEvaluationScores




   Base class for evaluation scores.

   .. py:attribute:: criteria
      :value: ['frizz_level', 'blipp_score']

      

   .. py:attribute:: frizz_level
      :type: float

      

   .. py:attribute:: blipp_score
      :type: float

      

   .. py:method:: higher_is_better(criterion: str) -> bool
      :staticmethod:

      Wether or not higher is better for this criterion.


   .. py:method:: bounds(criterion: str) -> Tuple[Union[int, float, None], Union[int, float, None]]
      :staticmethod:

      The bounds for this criterion


   .. py:method:: store_best(criterion: str) -> bool
      :staticmethod:

      Whether or not to save the best validation block and model
      weights for this criterion.



.. py:class:: DummyEvaluator




   Base class of all evaluators.

   An evaluator takes a post-processor's output and compares it against
   ground-truth.

   .. py:property:: score
      :type: dacapo.experiments.tasks.evaluators.dummy_evaluation_scores.DummyEvaluationScores


   .. py:attribute:: criteria
      :value: ['frizz_level', 'blipp_score']

      

   .. py:method:: evaluate(output_array_identifier, evaluation_dataset)

      Evaluate the given output array and dataset and returns the scores based on predefined criteria.

      :param output_array_identifier: The output array to be evaluated.
      :param evaluation_dataset: The dataset to be used for evaluation.

      :returns: An object of DummyEvaluationScores class, with the evaluation scores.
      :rtype: DummyEvaluationScore



.. py:class:: EvaluationScores


   Base class for evaluation scores.

   .. py:property:: criteria
      :type: List[str]
      :abstractmethod:


   .. py:method:: higher_is_better(criterion: str) -> bool
      :staticmethod:
      :abstractmethod:

      Wether or not higher is better for this criterion.


   .. py:method:: bounds(criterion: str) -> Tuple[Union[int, float, None], Union[int, float, None]]
      :staticmethod:
      :abstractmethod:

      The bounds for this criterion


   .. py:method:: store_best(criterion: str) -> bool
      :staticmethod:
      :abstractmethod:

      Whether or not to save the best validation block and model
      weights for this criterion.



.. py:class:: Evaluator




   Base class of all evaluators.

   An evaluator takes a post-processor's output and compares it against
   ground-truth.

   .. py:property:: best_scores
      :type: Dict[OutputIdentifier, BestScore]


   .. py:property:: criteria
      :type: List[str]
      :abstractmethod:

      A list of all criteria for which a model might be "best". i.e. your
      criteria might be "precision", "recall", and "jaccard". It is unlikely
      that the best iteration/post processing parameters will be the same
      for all 3 of these criteria

   .. py:property:: score
      :type: dacapo.experiments.tasks.evaluators.evaluation_scores.EvaluationScores
      :abstractmethod:


   .. py:method:: evaluate(output_array_identifier: dacapo.store.local_array_store.LocalArrayIdentifier, evaluation_array: dacapo.experiments.datasplits.datasets.arrays.Array) -> dacapo.experiments.tasks.evaluators.evaluation_scores.EvaluationScores
      :abstractmethod:

      Compares and evaluates the output array against the evaluation array.

      :param output_array_identifier: The output data array to evaluate
      :type output_array_identifier: Array
      :param evaluation_array: The evaluation data array to compare with the output
      :type evaluation_array: Array

      :returns: The detailed evaluation scores after the comparison.
      :rtype: EvaluationScores


   .. py:method:: is_best(dataset: dacapo.experiments.datasplits.datasets.Dataset, parameter: dacapo.experiments.tasks.post_processors.PostProcessorParameters, criterion: str, score: dacapo.experiments.tasks.evaluators.evaluation_scores.EvaluationScores) -> bool

      Check if the provided score is the best for this dataset/parameter/criterion combo


   .. py:method:: get_overall_best(dataset: dacapo.experiments.datasplits.datasets.Dataset, criterion: str)


   .. py:method:: get_overall_best_parameters(dataset: dacapo.experiments.datasplits.datasets.Dataset, criterion: str)


   .. py:method:: compare(score_1, score_2, criterion)


   .. py:method:: set_best(validation_scores: dacapo.experiments.validation_scores.ValidationScores) -> None

      Find the best iteration for each dataset/post_processing_parameter/criterion


   .. py:method:: higher_is_better(criterion: str) -> bool

      Wether or not higher is better for this criterion.


   .. py:method:: bounds(criterion: str) -> Tuple[Union[int, float, None], Union[int, float, None]]

      The bounds for this criterion


   .. py:method:: store_best(criterion: str) -> bool

      The bounds for this criterion



.. py:class:: MultiChannelBinarySegmentationEvaluationScores




   Base class for evaluation scores.

   .. py:property:: criteria


   .. py:attribute:: channel_scores
      :type: List[Tuple[str, BinarySegmentationEvaluationScores]]

      

   .. py:method:: higher_is_better(criterion: str) -> bool
      :staticmethod:

      Wether or not higher is better for this criterion.


   .. py:method:: store_best(criterion: str) -> bool
      :staticmethod:

      Whether or not to save the best validation block and model
      weights for this criterion.


   .. py:method:: bounds(criterion: str) -> Tuple[Union[int, float, None], Union[int, float, None]]
      :staticmethod:

      The bounds for this criterion



.. py:class:: BinarySegmentationEvaluationScores




   Base class for evaluation scores.

   .. py:attribute:: dice
      :type: float

      

   .. py:attribute:: jaccard
      :type: float

      

   .. py:attribute:: hausdorff
      :type: float

      

   .. py:attribute:: false_negative_rate
      :type: float

      

   .. py:attribute:: false_negative_rate_with_tolerance
      :type: float

      

   .. py:attribute:: false_positive_rate
      :type: float

      

   .. py:attribute:: false_discovery_rate
      :type: float

      

   .. py:attribute:: false_positive_rate_with_tolerance
      :type: float

      

   .. py:attribute:: voi
      :type: float

      

   .. py:attribute:: mean_false_distance
      :type: float

      

   .. py:attribute:: mean_false_negative_distance
      :type: float

      

   .. py:attribute:: mean_false_positive_distance
      :type: float

      

   .. py:attribute:: mean_false_distance_clipped
      :type: float

      

   .. py:attribute:: mean_false_negative_distance_clipped
      :type: float

      

   .. py:attribute:: mean_false_positive_distance_clipped
      :type: float

      

   .. py:attribute:: precision_with_tolerance
      :type: float

      

   .. py:attribute:: recall_with_tolerance
      :type: float

      

   .. py:attribute:: f1_score_with_tolerance
      :type: float

      

   .. py:attribute:: precision
      :type: float

      

   .. py:attribute:: recall
      :type: float

      

   .. py:attribute:: f1_score
      :type: float

      

   .. py:attribute:: criteria
      :value: ['dice', 'jaccard', 'hausdorff', 'false_negative_rate', 'false_negative_rate_with_tolerance',...

      

   .. py:method:: store_best(criterion: str) -> bool
      :staticmethod:

      Whether or not to save the best validation block and model
      weights for this criterion.


   .. py:method:: higher_is_better(criterion: str) -> bool
      :staticmethod:

      Wether or not higher is better for this criterion.


   .. py:method:: bounds(criterion: str) -> Tuple[Union[int, float, None], Union[int, float, None]]
      :staticmethod:

      The bounds for this criterion



.. py:class:: BinarySegmentationEvaluator(clip_distance: float, tol_distance: float, channels: List[str])




   Given a binary segmentation, compute various metrics to determine their similarity.

   .. py:property:: score


   .. py:attribute:: criteria
      :value: ['jaccard', 'voi']

      

   .. py:method:: evaluate(output_array_identifier, evaluation_array)

      Compares and evaluates the output array against the evaluation array.

      :param output_array_identifier: The output data array to evaluate
      :type output_array_identifier: Array
      :param evaluation_array: The evaluation data array to compare with the output
      :type evaluation_array: Array

      :returns: The detailed evaluation scores after the comparison.
      :rtype: EvaluationScores



.. py:class:: InstanceEvaluationScores




   Base class for evaluation scores.

   .. py:property:: voi


   .. py:attribute:: criteria
      :value: ['voi_split', 'voi_merge', 'voi']

      

   .. py:attribute:: voi_split
      :type: float

      

   .. py:attribute:: voi_merge
      :type: float

      

   .. py:method:: higher_is_better(criterion: str) -> bool
      :staticmethod:

      Wether or not higher is better for this criterion.


   .. py:method:: bounds(criterion: str) -> Tuple[Union[int, float, None], Union[int, float, None]]
      :staticmethod:

      The bounds for this criterion


   .. py:method:: store_best(criterion: str) -> bool
      :staticmethod:

      Whether or not to save the best validation block and model
      weights for this criterion.



.. py:class:: InstanceEvaluator




   Base class of all evaluators.

   An evaluator takes a post-processor's output and compares it against
   ground-truth.

   .. py:property:: score
      :type: dacapo.experiments.tasks.evaluators.instance_evaluation_scores.InstanceEvaluationScores


   .. py:attribute:: criteria
      :type: List[str]
      :value: ['voi_merge', 'voi_split', 'voi']

      

   .. py:method:: evaluate(output_array_identifier, evaluation_array)

      Compares and evaluates the output array against the evaluation array.

      :param output_array_identifier: The output data array to evaluate
      :type output_array_identifier: Array
      :param evaluation_array: The evaluation data array to compare with the output
      :type evaluation_array: Array

      :returns: The detailed evaluation scores after the comparison.
      :rtype: EvaluationScores



