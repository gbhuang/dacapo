:py:mod:`dacapo.experiments.tasks.evaluators`
=============================================

.. py:module:: dacapo.experiments.tasks.evaluators

.. autoapi-nested-parse::

   This script imports important classes from individual sub-modules into the package's root namespace which includes DummyEvaluationScores, DummyEvaluator, EvaluationScores,
   Evaluator, MultiChannelBinarySegmentationEvaluationScores, BinarySegmentationEvaluationScores, BinarySegmentationEvaluator, InstanceEvaluationScores, and InstanceEvaluator.

   These classes are used for different types of evaluation and scoring in the DACapo python library.

   Modules:
       - dummy_evaluation_scores: Contains the definition for DummyEvaluationScores Class.
       - dummy_evaluator: Contains the definition for DummyEvaluator Class.
       - evaluation_scores: Contains the definition for EvaluationScores Class.
       - evaluator: Contains the definition for Evaluator Class.
       - binary_segmentation_evaluation_scores: Contains the definition for MultiChannelBinarySegmentationEvaluationScores and BinarySegmentationEvaluationScores Classes.
       - binary_segmentation_evaluator: Contains the definition for BinarySegmentationEvaluator Class.
       - instance_evaluation_scores: Contains the definition for InstanceEvaluationScores Class.
       - instance_evaluator: Contains the definition for InstanceEvaluator Class.

   .. note:: - Import errors are ignored with `noqa` flag.



Submodules
----------
.. toctree::
   :titlesonly:
   :maxdepth: 1

   binary_segmentation_evaluation_scores/index.rst
   binary_segmentation_evaluator/index.rst
   dummy_evaluation_scores/index.rst
   dummy_evaluator/index.rst
   evaluation_scores/index.rst
   evaluator/index.rst
   instance_evaluation_scores/index.rst
   instance_evaluator/index.rst


Package Contents
----------------

Classes
~~~~~~~

.. autoapisummary::

   dacapo.experiments.tasks.evaluators.DummyEvaluationScores
   dacapo.experiments.tasks.evaluators.DummyEvaluator
   dacapo.experiments.tasks.evaluators.EvaluationScores
   dacapo.experiments.tasks.evaluators.Evaluator
   dacapo.experiments.tasks.evaluators.MultiChannelBinarySegmentationEvaluationScores
   dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores
   dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluator
   dacapo.experiments.tasks.evaluators.InstanceEvaluationScores
   dacapo.experiments.tasks.evaluators.InstanceEvaluator




.. py:class:: DummyEvaluationScores




   Base class for evaluation scores.

   .. py:attribute:: criteria
      :value: ['frizz_level', 'blipp_score']

      

   .. py:attribute:: frizz_level
      :type: float

      

   .. py:attribute:: blipp_score
      :type: float

      

   .. py:method:: higher_is_better(criterion: str) -> bool
      :staticmethod:

      Wether or not higher is better for this criterion.


   .. py:method:: bounds(criterion: str) -> Tuple[float, float]
      :staticmethod:

      The bounds for this criterion


   .. py:method:: store_best(criterion: str) -> bool
      :staticmethod:

      Whether or not to save the best validation block and model
      weights for this criterion.



.. py:class:: DummyEvaluator




   A Dummy Evaluator class which extends the Evaluator class for evaluation operations.

   .. attribute:: criteria

      List of evaluation criteria.

      :type: list

   .. py:property:: score
      :type: dacapo.experiments.tasks.evaluators.dummy_evaluation_scores.DummyEvaluationScores

      A property which is the instance of DummyEvaluationScores containing the evaluation scores.

      :returns: An object of DummyEvaluationScores class.
      :rtype: DummyEvaluationScores

   .. py:attribute:: criteria
      :value: ['frizz_level', 'blipp_score']

      

   .. py:method:: evaluate(output_array, evaluation_dataset)

      Evaluate the given output array and dataset and returns the scores based on predefined criteria.

      :param output_array: The output array to be evaluated.
      :param evaluation_dataset: The dataset to be used for evaluation.

      :returns: An object of DummyEvaluationScores class, with the evaluation scores.
      :rtype: DummyEvaluationScore



.. py:class:: EvaluationScores


   Base class for evaluation scores.

   .. py:property:: criteria
      :type: List[str]
      :abstractmethod:


   .. py:method:: higher_is_better(criterion: str) -> bool
      :staticmethod:
      :abstractmethod:

      Wether or not higher is better for this criterion.


   .. py:method:: bounds(criterion: str) -> Tuple[float, float]
      :staticmethod:
      :abstractmethod:

      The bounds for this criterion


   .. py:method:: store_best(criterion: str) -> bool
      :staticmethod:
      :abstractmethod:

      Whether or not to save the best validation block and model
      weights for this criterion.



.. py:class:: Evaluator




   Abstract base evaluator class. It provides the fundamental structure and methods for
   evaluators. A specific evaluator must inherent this class and implement its methods.

   .. attribute:: best_scores

      Dictionary storing the best scores, indexed by OutputIdentifier which is a tuple
      of Dataset, PostProcessorParameters, and criteria string.

      :type: Dict[OutputIdentifier, BestScore]

   .. py:property:: best_scores
      :type: Dict[OutputIdentifier, BestScore]

      Provides the best scores so far. If not available, an empty dictionary is
      created and returned.

   .. py:property:: criteria
      :type: List[str]
      :abstractmethod:

      A list of all criteria for which a model might be "best". i.e. your
      criteria might be "precision", "recall", and "jaccard". It is unlikely
      that the best iteration/post processing parameters will be the same
      for all 3 of these criteria

   .. py:property:: score
      :type: dacapo.experiments.tasks.evaluators.evaluation_scores.EvaluationScores
      :abstractmethod:

      The abstract property to get the overall score of the evaluation.

      :returns: The overall evaluation scores.
      :rtype: EvaluationScores

   .. py:method:: evaluate(output_array: dacapo.experiments.datasplits.datasets.arrays.Array, eval_array: dacapo.experiments.datasplits.datasets.arrays.Array) -> dacapo.experiments.tasks.evaluators.evaluation_scores.EvaluationScores
      :abstractmethod:

      Compares and evaluates the output array against the evaluation array.

      :param output_array: The output data array to evaluate
      :type output_array: Array
      :param eval_array: The evaluation data array to compare with the output
      :type eval_array: Array

      :returns: The detailed evaluation scores after the comparison.
      :rtype: EvaluationScores


   .. py:method:: is_best(dataset: dacapo.experiments.datasplits.datasets.Dataset, parameter: dacapo.experiments.tasks.post_processors.PostProcessorParameters, criterion: str, score: dacapo.experiments.tasks.evaluators.evaluation_scores.EvaluationScores) -> bool

      Determine if the provided score is the best for a specific
      dataset/parameter/criterion combination.

      :param dataset: The dataset for which the evaluation is done
      :type dataset: Dataset
      :param parameter: The post processing parameters used for the given dataset
      :type parameter: PostProcessorParameters
      :param criterion: The evaluation criterion
      :type criterion: str
      :param score: The calculated evaluation scores
      :type score: EvaluationScores

      :returns: True if the score is the best, False otherwise.
      :rtype: bool


   .. py:method:: set_best(validation_scores: dacapo.experiments.validation_scores.ValidationScores) -> None

      Identify the best iteration for each dataset/post_processing_parameter/criterion
      and set them as the current best scores.

      :param validation_scores: The validation scores from which the best are to be picked.
      :type validation_scores: ValidationScores


   .. py:method:: higher_is_better(criterion: str) -> bool

      Determines whether a higher score is better for the given criterion.

      :param criterion: The evaluation criterion
      :type criterion: str

      :returns: True if higher score is better, False otherwise.
      :rtype: bool


   .. py:method:: bounds(criterion: str) -> Tuple[float, float]

      Provides the bounds for the given evaluation criterion.

      :param criterion: The evaluation criterion
      :type criterion: str

      :returns: The lower and upper bounds for the criterion.
      :rtype: Tuple[float, float]


   .. py:method:: store_best(criterion: str) -> bool

      Determine if the best scores should be stored for the given criterion.

      :param criterion: The evaluation criterion
      :type criterion: str

      :returns: True if best scores should be stored, False otherwise.
      :rtype: bool



.. py:class:: MultiChannelBinarySegmentationEvaluationScores




   Base class for evaluation scores.

   .. py:property:: criteria


   .. py:attribute:: channel_scores
      :type: List[Tuple[str, BinarySegmentationEvaluationScores]]

      

   .. py:method:: higher_is_better(criterion: str) -> bool
      :staticmethod:

      Wether or not higher is better for this criterion.


   .. py:method:: store_best(criterion: str) -> bool
      :staticmethod:

      Whether or not to save the best validation block and model
      weights for this criterion.


   .. py:method:: bounds(criterion: str) -> Tuple[float, float]
      :staticmethod:

      The bounds for this criterion



.. py:class:: BinarySegmentationEvaluationScores




   Base class for evaluation scores.

   .. py:attribute:: dice
      :type: float

      

   .. py:attribute:: jaccard
      :type: float

      

   .. py:attribute:: hausdorff
      :type: float

      

   .. py:attribute:: false_negative_rate
      :type: float

      

   .. py:attribute:: false_negative_rate_with_tolerance
      :type: float

      

   .. py:attribute:: false_positive_rate
      :type: float

      

   .. py:attribute:: false_discovery_rate
      :type: float

      

   .. py:attribute:: false_positive_rate_with_tolerance
      :type: float

      

   .. py:attribute:: voi
      :type: float

      

   .. py:attribute:: mean_false_distance
      :type: float

      

   .. py:attribute:: mean_false_negative_distance
      :type: float

      

   .. py:attribute:: mean_false_positive_distance
      :type: float

      

   .. py:attribute:: mean_false_distance_clipped
      :type: float

      

   .. py:attribute:: mean_false_negative_distance_clipped
      :type: float

      

   .. py:attribute:: mean_false_positive_distance_clipped
      :type: float

      

   .. py:attribute:: precision_with_tolerance
      :type: float

      

   .. py:attribute:: recall_with_tolerance
      :type: float

      

   .. py:attribute:: f1_score_with_tolerance
      :type: float

      

   .. py:attribute:: precision
      :type: float

      

   .. py:attribute:: recall
      :type: float

      

   .. py:attribute:: f1_score
      :type: float

      

   .. py:attribute:: criteria
      :value: ['dice', 'jaccard', 'hausdorff', 'false_negative_rate', 'false_negative_rate_with_tolerance',...

      

   .. py:method:: store_best(criterion: str) -> bool
      :staticmethod:

      Whether or not to save the best validation block and model
      weights for this criterion.


   .. py:method:: higher_is_better(criterion: str) -> bool
      :staticmethod:

      Wether or not higher is better for this criterion.


   .. py:method:: bounds(criterion: str) -> Tuple[float, float]
      :staticmethod:

      The bounds for this criterion



.. py:class:: BinarySegmentationEvaluator(clip_distance: float, tol_distance: float, channels: List[str])




   Given a binary segmentation, compute various metrics to determine their similarity.

   .. py:property:: score

      The abstract property to get the overall score of the evaluation.

      :returns: The overall evaluation scores.
      :rtype: EvaluationScores

   .. py:attribute:: criteria
      :value: ['jaccard', 'voi']

      

   .. py:method:: evaluate(output_array_identifier, evaluation_array)

      Compares and evaluates the output array against the evaluation array.

      :param output_array: The output data array to evaluate
      :type output_array: Array
      :param eval_array: The evaluation data array to compare with the output
      :type eval_array: Array

      :returns: The detailed evaluation scores after the comparison.
      :rtype: EvaluationScores



.. py:class:: InstanceEvaluationScores




   Base class for evaluation scores.

   .. py:property:: voi


   .. py:attribute:: criteria
      :value: ['voi_split', 'voi_merge', 'voi']

      

   .. py:attribute:: voi_split
      :type: float

      

   .. py:attribute:: voi_merge
      :type: float

      

   .. py:method:: higher_is_better(criterion: str) -> bool
      :staticmethod:

      Wether or not higher is better for this criterion.


   .. py:method:: bounds(criterion: str) -> Tuple[float, float]
      :staticmethod:

      The bounds for this criterion


   .. py:method:: store_best(criterion: str) -> bool
      :staticmethod:

      Whether or not to save the best validation block and model
      weights for this criterion.



.. py:class:: InstanceEvaluator




   A subclass of Evaluator that specifically evaluates instance segmentation tasks. This class
   extends the base Evaluator class from dacapo library.

   .. attribute:: criteria

      A list of metric names that are used in this evaluation process.

      :type: list[str]

   .. py:property:: score
      :type: dacapo.experiments.tasks.evaluators.instance_evaluation_scores.InstanceEvaluationScores

      Property that returns the evaluation scores. However, currently, it only returns
      an empty InstanceEvaluationScores object.

      :returns: An object that supposedly contains evaluation scores.
      :rtype: InstanceEvaluationScores

   .. py:attribute:: criteria
      :value: ['voi_merge', 'voi_split', 'voi']

      

   .. py:method:: evaluate(output_array_identifier, evaluation_array)

      Evaluate the segmentation predictions with the ground truth data.

      :param output_array_identifier: A unique id that refers to the array that contains
                                      predicted labels from the segmentation.
      :param evaluation_array: The ground truth labels to compare the predicted labels with.

      :returns: An object that includes the segmentation evaluation results.
      :rtype: InstanceEvaluationScores



