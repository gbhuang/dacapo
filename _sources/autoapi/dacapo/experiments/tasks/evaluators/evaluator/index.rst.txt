:py:mod:`dacapo.experiments.tasks.evaluators.evaluator`
=======================================================

.. py:module:: dacapo.experiments.tasks.evaluators.evaluator


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   dacapo.experiments.tasks.evaluators.evaluator.Evaluator




Attributes
~~~~~~~~~~

.. autoapisummary::

   dacapo.experiments.tasks.evaluators.evaluator.OutputIdentifier
   dacapo.experiments.tasks.evaluators.evaluator.Iteration
   dacapo.experiments.tasks.evaluators.evaluator.Score
   dacapo.experiments.tasks.evaluators.evaluator.BestScore


.. py:data:: OutputIdentifier

   

.. py:data:: Iteration

   

.. py:data:: Score

   

.. py:data:: BestScore

   

.. py:class:: Evaluator




   Base class of all evaluators.

   An evaluator takes a post-processor's output and compares it against
   ground-truth.

   .. py:property:: best_scores
      :type: Dict[OutputIdentifier, BestScore]


   .. py:property:: criteria
      :type: List[str]
      :abstractmethod:

      A list of all criteria for which a model might be "best". i.e. your
      criteria might be "precision", "recall", and "jaccard". It is unlikely
      that the best iteration/post processing parameters will be the same
      for all 3 of these criteria

   .. py:property:: score
      :type: dacapo.experiments.tasks.evaluators.evaluation_scores.EvaluationScores
      :abstractmethod:


   .. py:method:: evaluate(output_array_identifier: dacapo.store.local_array_store.LocalArrayIdentifier, evaluation_array: dacapo.experiments.datasplits.datasets.arrays.Array) -> dacapo.experiments.tasks.evaluators.evaluation_scores.EvaluationScores
      :abstractmethod:

      Compares and evaluates the output array against the evaluation array.

      :param output_array_identifier: The output data array to evaluate
      :type output_array_identifier: Array
      :param evaluation_array: The evaluation data array to compare with the output
      :type evaluation_array: Array

      :returns: The detailed evaluation scores after the comparison.
      :rtype: EvaluationScores


   .. py:method:: is_best(dataset: dacapo.experiments.datasplits.datasets.Dataset, parameter: dacapo.experiments.tasks.post_processors.PostProcessorParameters, criterion: str, score: dacapo.experiments.tasks.evaluators.evaluation_scores.EvaluationScores) -> bool

      Check if the provided score is the best for this dataset/parameter/criterion combo


   .. py:method:: get_overall_best(dataset: dacapo.experiments.datasplits.datasets.Dataset, criterion: str)


   .. py:method:: get_overall_best_parameters(dataset: dacapo.experiments.datasplits.datasets.Dataset, criterion: str)


   .. py:method:: compare(score_1, score_2, criterion)


   .. py:method:: set_best(validation_scores: dacapo.experiments.validation_scores.ValidationScores) -> None

      Find the best iteration for each dataset/post_processing_parameter/criterion


   .. py:method:: higher_is_better(criterion: str) -> bool

      Wether or not higher is better for this criterion.


   .. py:method:: bounds(criterion: str) -> Tuple[Union[int, float, None], Union[int, float, None]]

      The bounds for this criterion


   .. py:method:: store_best(criterion: str) -> bool

      The bounds for this criterion



