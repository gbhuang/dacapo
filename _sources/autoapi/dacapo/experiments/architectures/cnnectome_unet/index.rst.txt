:py:mod:`dacapo.experiments.architectures.cnnectome_unet`
=========================================================

.. py:module:: dacapo.experiments.architectures.cnnectome_unet


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   dacapo.experiments.architectures.cnnectome_unet.CNNectomeUNet
   dacapo.experiments.architectures.cnnectome_unet.CNNectomeUNetModule
   dacapo.experiments.architectures.cnnectome_unet.ConvPass
   dacapo.experiments.architectures.cnnectome_unet.Downsample
   dacapo.experiments.architectures.cnnectome_unet.Upsample
   dacapo.experiments.architectures.cnnectome_unet.AttentionBlockModule




.. py:class:: CNNectomeUNet(architecture_config)




   An abstract base class for defining the architecture of a neural network model.
   It is inherited from PyTorch's Module and built-in class `ABC` (Abstract Base Classes).
   Other classes can inherit this class to define their own specific variations of architecture.
   It requires to implement several property methods, and also includes additional methods related to the architecture design.

   .. py:property:: eval_shape_increase

      Provides information about how much to increase the input shape during prediction.

      :returns: An instance representing the amount to increase in each dimension of the input shape.
      :rtype: Coordinate

   .. py:property:: input_shape

      Abstract method to define the spatial input shape for the neural network architecture.
      The shape should not account for the channels and batch dimensions.

      :returns: The spatial input shape.
      :rtype: Coordinate

   .. py:property:: num_in_channels
      :type: int

      Abstract method to return number of input channels required by the architecture.

      :returns: Required number of input channels.
      :rtype: int

   .. py:property:: num_out_channels
      :type: int

      Abstract method to return the number of output channels provided by the architecture.

      :returns: Number of output channels.
      :rtype: int

   .. py:method:: module()


   .. py:method:: scale(voxel_size)

      Method to scale the input voxel size as required by the architecture.

      :param input_voxel_size: The original size of the input voxel.
      :type input_voxel_size: Coordinate

      :returns: The scaled voxel size.
      :rtype: Coordinate


   .. py:method:: forward(x)



.. py:class:: CNNectomeUNetModule(in_channels, num_fmaps, fmap_inc_factor, downsample_factors, kernel_size_down=None, kernel_size_up=None, activation='ReLU', num_fmaps_out=None, num_heads=1, constant_upsample=False, padding='valid', upsample_channel_contraction=False, activation_on_upsample=False, use_attention=False)




   Base class for all neural network modules.

   Your models should also subclass this class.

   Modules can also contain other Modules, allowing to nest them in
   a tree structure. You can assign the submodules as regular attributes::

       import torch.nn as nn
       import torch.nn.functional as F

       class Model(nn.Module):
           def __init__(self):
               super().__init__()
               self.conv1 = nn.Conv2d(1, 20, 5)
               self.conv2 = nn.Conv2d(20, 20, 5)

           def forward(self, x):
               x = F.relu(self.conv1(x))
               return F.relu(self.conv2(x))

   Submodules assigned in this way will be registered, and will have their
   parameters converted too when you call :meth:`to`, etc.

   .. note::
       As per the example above, an ``__init__()`` call to the parent class
       must be made before assignment on the child.

   :ivar training: Boolean represents whether this module is in training or
                   evaluation mode.
   :vartype training: bool

   .. py:method:: rec_forward(level, f_in)


   .. py:method:: forward(x)



.. py:class:: ConvPass(in_channels, out_channels, kernel_sizes, activation, padding='valid')




   Base class for all neural network modules.

   Your models should also subclass this class.

   Modules can also contain other Modules, allowing to nest them in
   a tree structure. You can assign the submodules as regular attributes::

       import torch.nn as nn
       import torch.nn.functional as F

       class Model(nn.Module):
           def __init__(self):
               super().__init__()
               self.conv1 = nn.Conv2d(1, 20, 5)
               self.conv2 = nn.Conv2d(20, 20, 5)

           def forward(self, x):
               x = F.relu(self.conv1(x))
               return F.relu(self.conv2(x))

   Submodules assigned in this way will be registered, and will have their
   parameters converted too when you call :meth:`to`, etc.

   .. note::
       As per the example above, an ``__init__()`` call to the parent class
       must be made before assignment on the child.

   :ivar training: Boolean represents whether this module is in training or
                   evaluation mode.
   :vartype training: bool

   .. py:method:: forward(x)



.. py:class:: Downsample(downsample_factor)




   Base class for all neural network modules.

   Your models should also subclass this class.

   Modules can also contain other Modules, allowing to nest them in
   a tree structure. You can assign the submodules as regular attributes::

       import torch.nn as nn
       import torch.nn.functional as F

       class Model(nn.Module):
           def __init__(self):
               super().__init__()
               self.conv1 = nn.Conv2d(1, 20, 5)
               self.conv2 = nn.Conv2d(20, 20, 5)

           def forward(self, x):
               x = F.relu(self.conv1(x))
               return F.relu(self.conv2(x))

   Submodules assigned in this way will be registered, and will have their
   parameters converted too when you call :meth:`to`, etc.

   .. note::
       As per the example above, an ``__init__()`` call to the parent class
       must be made before assignment on the child.

   :ivar training: Boolean represents whether this module is in training or
                   evaluation mode.
   :vartype training: bool

   .. py:method:: forward(x)



.. py:class:: Upsample(scale_factor, mode='transposed_conv', in_channels=None, out_channels=None, crop_factor=None, next_conv_kernel_sizes=None, activation=None)




   Base class for all neural network modules.

   Your models should also subclass this class.

   Modules can also contain other Modules, allowing to nest them in
   a tree structure. You can assign the submodules as regular attributes::

       import torch.nn as nn
       import torch.nn.functional as F

       class Model(nn.Module):
           def __init__(self):
               super().__init__()
               self.conv1 = nn.Conv2d(1, 20, 5)
               self.conv2 = nn.Conv2d(20, 20, 5)

           def forward(self, x):
               x = F.relu(self.conv1(x))
               return F.relu(self.conv2(x))

   Submodules assigned in this way will be registered, and will have their
   parameters converted too when you call :meth:`to`, etc.

   .. note::
       As per the example above, an ``__init__()`` call to the parent class
       must be made before assignment on the child.

   :ivar training: Boolean represents whether this module is in training or
                   evaluation mode.
   :vartype training: bool

   .. py:method:: crop_to_factor(x, factor, kernel_sizes)

      Crop feature maps to ensure translation equivariance with stride of
      upsampling factor. This should be done right after upsampling, before
      application of the convolutions with the given kernel sizes.

      The crop could be done after the convolutions, but it is more efficient
      to do that before (feature maps will be smaller).


   .. py:method:: crop(x, shape)

      Center-crop x to match spatial dimensions given by shape.


   .. py:method:: forward(g_out, f_left=None)



.. py:class:: AttentionBlockModule(F_g, F_l, F_int, dims, upsample_factor=None)




   Base class for all neural network modules.

   Your models should also subclass this class.

   Modules can also contain other Modules, allowing to nest them in
   a tree structure. You can assign the submodules as regular attributes::

       import torch.nn as nn
       import torch.nn.functional as F

       class Model(nn.Module):
           def __init__(self):
               super().__init__()
               self.conv1 = nn.Conv2d(1, 20, 5)
               self.conv2 = nn.Conv2d(20, 20, 5)

           def forward(self, x):
               x = F.relu(self.conv1(x))
               return F.relu(self.conv2(x))

   Submodules assigned in this way will be registered, and will have their
   parameters converted too when you call :meth:`to`, etc.

   .. note::
       As per the example above, an ``__init__()`` call to the parent class
       must be made before assignment on the child.

   :ivar training: Boolean represents whether this module is in training or
                   evaluation mode.
   :vartype training: bool

   .. py:method:: calculate_and_apply_padding(smaller_tensor, larger_tensor)

      Calculate and apply symmetric padding to the smaller tensor to match the dimensions of the larger tensor.

      Args:
      smaller_tensor (Tensor): The tensor to be padded.
      larger_tensor (Tensor): The tensor whose dimensions the smaller tensor needs to match.

      Returns:
      Tensor: The padded smaller tensor with the same dimensions as the larger tensor.


   .. py:method:: forward(g, x)



