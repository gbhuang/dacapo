:py:mod:`dacapo.experiments.architectures.cnnectome_unet`
=========================================================

.. py:module:: dacapo.experiments.architectures.cnnectome_unet


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   dacapo.experiments.architectures.cnnectome_unet.CNNectomeUNet
   dacapo.experiments.architectures.cnnectome_unet.CNNectomeUNetModule
   dacapo.experiments.architectures.cnnectome_unet.ConvPass
   dacapo.experiments.architectures.cnnectome_unet.Downsample
   dacapo.experiments.architectures.cnnectome_unet.Upsample
   dacapo.experiments.architectures.cnnectome_unet.AttentionBlockModule




.. py:class:: CNNectomeUNet(architecture_config)




   Base class for all neural network modules.

   Your models should also subclass this class.

   Modules can also contain other Modules, allowing to nest them in
   a tree structure. You can assign the submodules as regular attributes::

       import torch.nn as nn
       import torch.nn.functional as F

       class Model(nn.Module):
           def __init__(self):
               super().__init__()
               self.conv1 = nn.Conv2d(1, 20, 5)
               self.conv2 = nn.Conv2d(20, 20, 5)

           def forward(self, x):
               x = F.relu(self.conv1(x))
               return F.relu(self.conv2(x))

   Submodules assigned in this way will be registered, and will have their
   parameters converted too when you call :meth:`to`, etc.

   .. note::
       As per the example above, an ``__init__()`` call to the parent class
       must be made before assignment on the child.

   :ivar training: Boolean represents whether this module is in training or
                   evaluation mode.
   :vartype training: bool

   .. py:property:: eval_shape_increase

      How much to increase the input shape during prediction.

   .. py:property:: input_shape

      The spatial input shape (i.e., not accounting for channels and batch
      dimensions) of this architecture.

   .. py:property:: num_in_channels
      :type: int

      Return the number of input channels this architecture expects.

   .. py:property:: num_out_channels
      :type: int

      Return the number of output channels of this architecture.

   .. py:method:: module()


   .. py:method:: scale(voxel_size)


   .. py:method:: forward(x)



.. py:class:: CNNectomeUNetModule(in_channels, num_fmaps, fmap_inc_factor, downsample_factors, kernel_size_down=None, kernel_size_up=None, activation='ReLU', num_fmaps_out=None, num_heads=1, constant_upsample=False, padding='valid', upsample_channel_contraction=False, activation_on_upsample=False, use_attention=False)




   Base class for all neural network modules.

   Your models should also subclass this class.

   Modules can also contain other Modules, allowing to nest them in
   a tree structure. You can assign the submodules as regular attributes::

       import torch.nn as nn
       import torch.nn.functional as F

       class Model(nn.Module):
           def __init__(self):
               super().__init__()
               self.conv1 = nn.Conv2d(1, 20, 5)
               self.conv2 = nn.Conv2d(20, 20, 5)

           def forward(self, x):
               x = F.relu(self.conv1(x))
               return F.relu(self.conv2(x))

   Submodules assigned in this way will be registered, and will have their
   parameters converted too when you call :meth:`to`, etc.

   .. note::
       As per the example above, an ``__init__()`` call to the parent class
       must be made before assignment on the child.

   :ivar training: Boolean represents whether this module is in training or
                   evaluation mode.
   :vartype training: bool

   .. py:method:: rec_forward(level, f_in)


   .. py:method:: forward(x)



.. py:class:: ConvPass(in_channels, out_channels, kernel_sizes, activation, padding='valid')




   Base class for all neural network modules.

   Your models should also subclass this class.

   Modules can also contain other Modules, allowing to nest them in
   a tree structure. You can assign the submodules as regular attributes::

       import torch.nn as nn
       import torch.nn.functional as F

       class Model(nn.Module):
           def __init__(self):
               super().__init__()
               self.conv1 = nn.Conv2d(1, 20, 5)
               self.conv2 = nn.Conv2d(20, 20, 5)

           def forward(self, x):
               x = F.relu(self.conv1(x))
               return F.relu(self.conv2(x))

   Submodules assigned in this way will be registered, and will have their
   parameters converted too when you call :meth:`to`, etc.

   .. note::
       As per the example above, an ``__init__()`` call to the parent class
       must be made before assignment on the child.

   :ivar training: Boolean represents whether this module is in training or
                   evaluation mode.
   :vartype training: bool

   .. py:method:: forward(x)



.. py:class:: Downsample(downsample_factor)




   Base class for all neural network modules.

   Your models should also subclass this class.

   Modules can also contain other Modules, allowing to nest them in
   a tree structure. You can assign the submodules as regular attributes::

       import torch.nn as nn
       import torch.nn.functional as F

       class Model(nn.Module):
           def __init__(self):
               super().__init__()
               self.conv1 = nn.Conv2d(1, 20, 5)
               self.conv2 = nn.Conv2d(20, 20, 5)

           def forward(self, x):
               x = F.relu(self.conv1(x))
               return F.relu(self.conv2(x))

   Submodules assigned in this way will be registered, and will have their
   parameters converted too when you call :meth:`to`, etc.

   .. note::
       As per the example above, an ``__init__()`` call to the parent class
       must be made before assignment on the child.

   :ivar training: Boolean represents whether this module is in training or
                   evaluation mode.
   :vartype training: bool

   .. py:method:: forward(x)



.. py:class:: Upsample(scale_factor, mode='transposed_conv', in_channels=None, out_channels=None, crop_factor=None, next_conv_kernel_sizes=None, activation=None)




   Base class for all neural network modules.

   Your models should also subclass this class.

   Modules can also contain other Modules, allowing to nest them in
   a tree structure. You can assign the submodules as regular attributes::

       import torch.nn as nn
       import torch.nn.functional as F

       class Model(nn.Module):
           def __init__(self):
               super().__init__()
               self.conv1 = nn.Conv2d(1, 20, 5)
               self.conv2 = nn.Conv2d(20, 20, 5)

           def forward(self, x):
               x = F.relu(self.conv1(x))
               return F.relu(self.conv2(x))

   Submodules assigned in this way will be registered, and will have their
   parameters converted too when you call :meth:`to`, etc.

   .. note::
       As per the example above, an ``__init__()`` call to the parent class
       must be made before assignment on the child.

   :ivar training: Boolean represents whether this module is in training or
                   evaluation mode.
   :vartype training: bool

   .. py:method:: crop_to_factor(x, factor, kernel_sizes)

      Crop feature maps to ensure translation equivariance with stride of
      upsampling factor. This should be done right after upsampling, before
      application of the convolutions with the given kernel sizes.

      The crop could be done after the convolutions, but it is more efficient
      to do that before (feature maps will be smaller).


   .. py:method:: crop(x, shape)

      Center-crop x to match spatial dimensions given by shape.


   .. py:method:: forward(g_out, f_left=None)



.. py:class:: AttentionBlockModule(F_g, F_l, F_int, dims, upsample_factor=None)




   Base class for all neural network modules.

   Your models should also subclass this class.

   Modules can also contain other Modules, allowing to nest them in
   a tree structure. You can assign the submodules as regular attributes::

       import torch.nn as nn
       import torch.nn.functional as F

       class Model(nn.Module):
           def __init__(self):
               super().__init__()
               self.conv1 = nn.Conv2d(1, 20, 5)
               self.conv2 = nn.Conv2d(20, 20, 5)

           def forward(self, x):
               x = F.relu(self.conv1(x))
               return F.relu(self.conv2(x))

   Submodules assigned in this way will be registered, and will have their
   parameters converted too when you call :meth:`to`, etc.

   .. note::
       As per the example above, an ``__init__()`` call to the parent class
       must be made before assignment on the child.

   :ivar training: Boolean represents whether this module is in training or
                   evaluation mode.
   :vartype training: bool

   .. py:method:: calculate_and_apply_padding(smaller_tensor, larger_tensor)

      Calculate and apply symmetric padding to the smaller tensor to match the dimensions of the larger tensor.

      Args:
      smaller_tensor (Tensor): The tensor to be padded.
      larger_tensor (Tensor): The tensor whose dimensions the smaller tensor needs to match.

      Returns:
      Tensor: The padded smaller tensor with the same dimensions as the larger tensor.


   .. py:method:: forward(g, x)



