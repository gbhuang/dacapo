:py:mod:`dacapo.experiments`
============================

.. py:module:: dacapo.experiments


Subpackages
-----------
.. toctree::
   :titlesonly:
   :maxdepth: 3

   architectures/index.rst
   arraytypes/index.rst
   datasplits/index.rst
   starts/index.rst
   tasks/index.rst
   trainers/index.rst


Submodules
----------
.. toctree::
   :titlesonly:
   :maxdepth: 1

   model/index.rst
   run/index.rst
   run_config/index.rst
   training_iteration_stats/index.rst
   training_stats/index.rst
   validation_iteration_scores/index.rst
   validation_scores/index.rst


Package Contents
----------------

Classes
~~~~~~~

.. autoapisummary::

   dacapo.experiments.Model
   dacapo.experiments.RunConfig
   dacapo.experiments.TrainingIterationStats
   dacapo.experiments.TrainingStats
   dacapo.experiments.ValidationIterationScores
   dacapo.experiments.ValidationScores




.. py:class:: Model(architecture: dacapo.experiments.architectures.architecture.Architecture, prediction_head: torch.nn.Module, eval_activation: torch.nn.Module | None = None)




   A trainable DaCapo model. Consists of an ``Architecture`` and a
   prediction head. Models are generated by ``Predictor``s.

   May include an optional eval_activation that is only executed when the model
   is in eval mode. This is particularly useful if you want to train with something
   like BCELossWithLogits, since you want to avoid applying softmax while training,
   but apply it during evaluation.

   .. py:attribute:: num_out_channels
      :type: int

      

   .. py:attribute:: num_in_channels
      :type: int

      

   .. py:method:: forward(x)


   .. py:method:: compute_output_shape(input_shape: funlib.geometry.Coordinate) -> Tuple[int, funlib.geometry.Coordinate]

      Compute the spatial shape (i.e., not accounting for channels and
      batch dimensions) of this model, when fed a tensor of the given spatial
      shape as input.


   .. py:method:: scale(voxel_size: funlib.geometry.Coordinate) -> funlib.geometry.Coordinate



.. py:class:: RunConfig


   .. py:attribute:: task_config
      :type: dacapo.experiments.tasks.TaskConfig

      

   .. py:attribute:: architecture_config
      :type: dacapo.experiments.architectures.ArchitectureConfig

      

   .. py:attribute:: trainer_config
      :type: dacapo.experiments.trainers.TrainerConfig

      

   .. py:attribute:: datasplit_config
      :type: dacapo.experiments.datasplits.DataSplitConfig

      

   .. py:attribute:: name
      :type: str

      

   .. py:attribute:: repetition
      :type: int

      

   .. py:attribute:: num_iterations
      :type: int

      

   .. py:attribute:: validation_interval
      :type: int

      

   .. py:attribute:: start_config
      :type: Optional[dacapo.experiments.starts.StartConfig]

      


.. py:class:: TrainingIterationStats


   .. py:attribute:: iteration
      :type: int

      

   .. py:attribute:: loss
      :type: float

      

   .. py:attribute:: time
      :type: float

      


.. py:class:: TrainingStats


   .. py:attribute:: iteration_stats
      :type: List[dacapo.experiments.training_iteration_stats.TrainingIterationStats]

      

   .. py:method:: add_iteration_stats(iteration_stats: dacapo.experiments.training_iteration_stats.TrainingIterationStats) -> None


   .. py:method:: delete_after(iteration: int) -> None


   .. py:method:: trained_until() -> int

      The number of iterations trained for (the maximum iteration plus
      one).
      0 if no iterations trained yet.


   .. py:method:: to_xarray() -> xarray.DataArray



.. py:class:: ValidationIterationScores


   .. py:attribute:: iteration
      :type: int

      

   .. py:attribute:: scores
      :type: List[List[List[float]]]

      


.. py:class:: ValidationScores


   .. py:property:: criteria
      :type: List[str]


   .. py:property:: parameter_names
      :type: List[str]


   .. py:attribute:: parameters
      :type: List[dacapo.experiments.tasks.post_processors.PostProcessorParameters]

      

   .. py:attribute:: datasets
      :type: List[dacapo.experiments.datasplits.datasets.Dataset]

      

   .. py:attribute:: evaluation_scores
      :type: dacapo.experiments.tasks.evaluators.EvaluationScores

      

   .. py:attribute:: scores
      :type: List[dacapo.experiments.validation_iteration_scores.ValidationIterationScores]

      

   .. py:method:: subscores(iteration_scores: List[dacapo.experiments.validation_iteration_scores.ValidationIterationScores]) -> ValidationScores


   .. py:method:: add_iteration_scores(iteration_scores: dacapo.experiments.validation_iteration_scores.ValidationIterationScores) -> None


   .. py:method:: delete_after(iteration: int) -> None


   .. py:method:: validated_until() -> int

      The number of iterations validated for (the maximum iteration plus
      one).


   .. py:method:: compare(existing_iteration_scores: List[dacapo.experiments.validation_iteration_scores.ValidationIterationScores]) -> Tuple[bool, int]

      Compares iteration stats provided from elsewhere to scores we have saved locally.
      Local scores take priority. If local scores are at a lower iteration than the
      existing ones, delete the existing ones and replace with local.
      If local iteration > existing iteration, just update existing scores with the last
      overhanging local scores.


   .. py:method:: to_xarray() -> xarray.DataArray


   .. py:method:: get_best(data: xarray.DataArray, dim: str) -> Tuple[xarray.DataArray, xarray.DataArray]

      Compute the Best scores along dimension "dim" per criterion.
      Returns both the index associated with the best value, and the
      best value in two seperate arrays.



